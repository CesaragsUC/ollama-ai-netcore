#  âœ¨ Ollama AI PoC â€” .NET 9 (API) + Angular (Web)

## PoC de chat com IA local usando Ollama
 com dois modelos:

**phi3** â†’ usado para texto/arquivos (ex.: PDF â†’ texto).
https://ollama.com/library/phi3

**llama3.2-vision** â†’ usado para imagens (multimodal).
https://ollama.com/library/llama3.2-vision

O frontend consome respostas via streaming (.NET API) para mostrar a saÃ­da token a token com efeito de digitaÃ§Ã£o.

ğŸ› ï¸ Configuration:
```

#1 Instale o Ollama
Aqui: https://ollama.com/

#2 Baixe os modelos
ollama pull phi3:3.8b
ollama pull llama3.2-vision:11b-instruct-q4_K_M
```
### ğŸ” Como se Ollama esta rodando:
- http://localhost:11434/
<img width="375" height="164" alt="image" src="https://github.com/user-attachments/assets/d90607ff-0331-4a99-a64d-e564a06648dc" />

### ğŸ“Œ Parametros do modelo llama3.2-vision:

- **`q4_K_M`**  -> Ã‰ o tipo de quantizaÃ§Ã£o do modelo (formato GGUF do llama.cpp).
- **`11b`** -> ~11 bilhÃµes de parÃ¢metros
- **`instruct`**  â†’ afinado para seguir instruÃ§Ãµes
- **`q4_K_M `**  â†’ K-quant 4-bit, variante â€œMâ€ (medium)

### ğŸ” O que isso significa na prÃ¡tica:

- q4 em mÃ©dia ~4 bits por peso (bem comprimido).
- Menos RAM/VRAM usada e geraÃ§Ã£o mais rÃ¡pida.
- Qualidade um pouco abaixo de quantizaÃ§Ãµes menos agressivas (q5/q6/q8).
- K â†’ â€œK-quantsâ€, uma famÃ­lia de quantizaÃ§Ãµes mais modernas do llama.cpp (melhor relaÃ§Ã£o qualidadeÃ—memÃ³ria que as antigas q4_0/q4_1).
- M â†’ â€œMediumâ€: perfil equilibrado de qualidade vs. uso de memÃ³ria.
- q4_K_S (Small) usa um pouco menos de memÃ³ria, perde mais qualidade.
- q4_K_M Ã© o equilÃ­brio recomendado em 4-bit.
- Se quiser mais fidelidade (e tiver memÃ³ria), use q5_K_M / q6_K / q8_0.

### ğŸ”§ Como usar outras quantizaÃ§Ãµes no Ollama:
````
# baixar outra quantizaÃ§Ã£o
ollama pull llama3.2-vision:11b-instruct-q5_K_M

# rodar especificando a tag desejada
ollama run llama3.2-vision:11b-instruct-q5_K_M

````

## Arquitetura:

```
Angular (UI + API)
  â”œâ”€â”€ POST /api/chat/send-stream          â†’ texto  (usa phi3)
  â””â”€â”€ POST /api/chat/send-stream-files    â†’ arquivos/imagens
                                           â€¢ PDF â†’ texto â†’ phi3
                                           â€¢ image/* â†’ llama3.2-vision

Ollama (localhost:11434)
  â”œâ”€â”€ phi3:3.8b
  â””â”€â”€ llama3.2-vision:11b-instruct-q4_K_M
```

## Comandos Ãºteis (Ollama)

````
# listar modelos instalados
ollama list

# detalhes de um modelo
ollama show llama3.2-vision:11b-instruct-q4_K_M

# rodar um modelo manualmente (teste rÃ¡pido)
ollama run phi3:3.8b

# remover um modelo
ollama rm phi3:3.8b

````
